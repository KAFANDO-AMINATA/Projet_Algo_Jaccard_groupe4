{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "# Téléchargement les ressources NLTK \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence, remove_stopwords=True, use_lemmatization=True):\n",
        "    \"\"\"\n",
        "    Prétraitement d'une phrase :\n",
        "    - minuscules\n",
        "    - suppression ponctuation\n",
        "    - stopwords\n",
        "    - lemmatisation\n",
        "    \"\"\"\n",
        "    if not sentence:\n",
        "        return []\n",
        "\n",
        "    # Minuscules + suppression ponctuation\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    sentence = sentence.lower().translate(translator)\n",
        "\n",
        "    # Tokenisation\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Stopwords anglais\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if w not in stop_words]\n",
        "\n",
        "    # Lemmatisation\n",
        "    if use_lemmatization:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def are_synonyms(word1, word2):\n",
        "    \"\"\"\n",
        "    Vérifie si deux mots sont synonymes avec WordNet\n",
        "    \"\"\"\n",
        "    if word1 == word2:\n",
        "        return True\n",
        "\n",
        "    synsets1 = wordnet.synsets(word1)\n",
        "    synsets2 = wordnet.synsets(word2)\n",
        "\n",
        "    if not synsets1 or not synsets2:\n",
        "        return False\n",
        "\n",
        "    for syn1 in synsets1:\n",
        "        for syn2 in synsets2:\n",
        "            # Même synset → mots synonymes\n",
        "            if syn1 == syn2:\n",
        "                return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard_similarity_sentences(sentence1, sentence2, remove_stopwords=True, use_lemmatization=True):\n",
        "    \"\"\"\n",
        "    Calcule la similarité de Jaccard entre deux phrases \n",
        "    - prend en compte les répétitions de mots (Counter)\n",
        "    - gère les synonymes via WordNet\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prétraitement\n",
        "        words1 = preprocess_sentence(sentence1, remove_stopwords, use_lemmatization)\n",
        "        words2 = preprocess_sentence(sentence2, remove_stopwords, use_lemmatization)\n",
        "\n",
        "        if not words1 or not words2:\n",
        "            return 0.0\n",
        "\n",
        "        counter1 = Counter(words1)\n",
        "        counter2 = Counter(words2)\n",
        "\n",
        "        # Gestion des synonymes dans l'intersection\n",
        "        intersection_count = 0\n",
        "        used_pairs = set()  \n",
        "\n",
        "        for w1 in counter1:\n",
        "            for w2 in counter2:\n",
        "                if are_synonyms(w1, w2) and (w1, w2) not in used_pairs:\n",
        "                    intersection_count += min(counter1[w1], counter2[w2])\n",
        "                    used_pairs.add((w1, w2))\n",
        "                    break  \n",
        "\n",
        "        # Union pondérée\n",
        "        union_count = sum(counter1.values()) + sum(counter2.values()) - intersection_count\n",
        "\n",
        "        return intersection_count / union_count if union_count else 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du calcul : {e}\")\n",
        "        return 0.0\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
